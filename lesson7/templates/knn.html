{% extends "base.html" %}

{% block header %}
<h1>職能發展學院</h1>
{% endblock %}

{% block content %}
<h1>KNN分類</h1>
<p>K 鄰近演算法 (KNN) 是一種簡單且常用的非參數機器學習分類演算法。它的核心思想是「物以類聚」，即一個資料點的類別由其K個最接近的鄰居的類別決定。在分類時，KNN 會計算新樣本與所有訓練樣本的距離 (例如歐氏距離)，找出距離最近的 K 個鄰居，然後根據這 K 個鄰居的多數投票來決定新樣本的最終類別。由於訓練階段僅需儲存資料集，因此 KNN 是一種懶惰學習 (Lazy Learning) 的方法。</p>
<pre>
    [ 1. 蒐集並準備資料 ]
    (包含特徵 (X) 與類別標籤 (y) 的完整訓練集)
    |
    v
    [ 2. 選擇 K 值 ]
    (K 為一個超參數，代表要考慮的鄰居數量)
    |
    v
    [ 3. 儲存整個訓練集 ]
    (這是 KNN 的「學習」階段：它只是懶惰地記住所有資料點)
    |
    v
    [ 4. 接收新的待分類資料點 (X_new) ]
    (開始進行預測 / 推論)
    |
    v
    [ 5. 計算距離 ]
    (計算 X_new 與所有訓練集中資料點的距離 (常用歐氏距離))
    |
    v
    [ 6. 找出 K 個最近的鄰居 ]
    (根據距離結果，選出 K 個距離 X_new 最近的點)
    |
    v
    [ 7. 多數投票決定類別 ]
    (統計這 K 個鄰居的類別標籤，將佔多數的類別指定為 X_new 的預測結果)
    |
    +------> [ 準確度不佳？ ] ---+
    |                          |
    v                          | (回到第2步，調整 K 值，或回到第5步，
    [ 8. 輸出分類結果與應用 ] <---+ 嘗試不同的距離計算方法) (將預測結果應用到實際場景中)
</pre>
{% endblock %}